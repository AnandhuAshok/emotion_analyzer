{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/main_project/Audioo/Test_audio/a02.wav\n",
      "x:\n",
      "/home/user/main_project/Audioo/Test_audio/a02.wav\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "[[3.25285587e-37 0.00000000e+00 0.00000000e+00 1.08377126e-29\n",
      "  0.00000000e+00 9.99998212e-01 3.28040581e-23 5.52000390e-10\n",
      "  1.77677759e-06 3.53654777e-08]]\n",
      "male_angry\n",
      "/home/user/main_project/Audioo/Test_audio/h09.wav\n",
      "x:\n",
      "/home/user/main_project/Audioo/Test_audio/h09.wav\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "[[4.7420530e-34 0.0000000e+00 0.0000000e+00 1.5200334e-35 3.5276728e-38\n",
      "  1.7607232e-04 6.8674724e-21 3.1131334e-10 9.9982399e-01 2.0347628e-09]]\n",
      "male_happy\n",
      "/home/user/main_project/Audioo/Test_audio/sa14.wav\n",
      "x:\n",
      "/home/user/main_project/Audioo/Test_audio/sa14.wav\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "[[5.2340327e-37 0.0000000e+00 7.5734976e-36 8.2576100e-34 0.0000000e+00\n",
      "  7.0950676e-08 3.1555166e-27 3.3541884e-13 2.7235287e-11 9.9999988e-01]]\n",
      "male_sad\n",
      "/home/user/main_project/Audioo/Test_audio/sa14.wav\n",
      "x:\n",
      "/home/user/main_project/Audioo/Test_audio/sa14.wav\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "[[5.2340327e-37 0.0000000e+00 7.5734976e-36 8.2576100e-34 0.0000000e+00\n",
      "  7.0950676e-08 3.1555166e-27 3.3541884e-13 2.7235287e-11 9.9999988e-01]]\n",
      "male_sad\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "\n",
    "import PIL\n",
    "\n",
    "from tkinter import filedialog\n",
    "master = Tk()\n",
    "var = StringVar()\n",
    "emo = StringVar()\n",
    "fame = StringVar()\n",
    "master.title('EMOTION DETECTION')\n",
    "##master.overrideredirect(True)\n",
    "master.geometry(\"{0}x{1}+0+0\".format(master.winfo_screenwidth(), master.winfo_screenheight()))\n",
    "\n",
    "\n",
    "topFrame = tk.Frame(master, bg = 'black', width = 1500, height=100, relief = 'raised') # , padx = 100, pady=100\n",
    "topFrame.grid(row = 0, column = 0, columnspan = 3,  sticky=\"w\")\n",
    "\n",
    "\n",
    "\n",
    "LFrame = tk.Frame(master, width = 800, height = 900, relief = 'raised') # , padx = 100, pady=100\n",
    "LFrame.grid(row = 2, column = 0,  sticky=\"nsew\")\n",
    "\n",
    "RFrame = tk.Frame(master, width = 700, height = 100, relief = 'raised') # , padx = 100, pady=100\n",
    "RFrame.grid(row = 2, column = 1,  sticky=\"nsew\")\n",
    "\n",
    "\n",
    "def callback():\n",
    "    import pyaudio\n",
    "    import wave\n",
    "\n",
    "    CHUNK = 1024 \n",
    "    FORMAT = pyaudio.paInt16 #paInt8\n",
    "    CHANNELS = 2 \n",
    "    RATE = 44100 #sample rate\n",
    "    RECORD_SECONDS = 4\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    " \n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "def findemo():\n",
    "   \n",
    "    import pandas as pd\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    from keras.utils import np_utils\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from PIL import Image, ImageTk\n",
    "    from keras.models import load_model\n",
    "    lmodel=load_model(\"Emotion_Voice_Detection_Model.h5\")\n",
    "    \n",
    "    lb = LabelEncoder()\n",
    "    lb.classes_ = np.load('classes.npy')\n",
    "   \n",
    "    X, sample_rate = librosa.load(master.filename, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "    \n",
    "    featurelive = mfccs\n",
    "    livedf2 = featurelive\n",
    "    livedf2= pd.DataFrame(data=livedf2)\n",
    "    livedf2 = livedf2.stack().to_frame().T \n",
    "    \n",
    "    twodim= np.expand_dims(livedf2, axis=2)\n",
    "    \n",
    "    livepreds = lmodel.predict(twodim,batch_size=32,verbose=1)\n",
    "    livepreds1=livepreds.argmax(axis=1)\n",
    "    liveabc = livepreds1.astype(int).flatten()\n",
    "    print(livepreds)\n",
    "    livepredictions = (lb.inverse_transform((liveabc)))\n",
    "    print(livepredictions[0])\n",
    "    emo.set(livepredictions[0])  \n",
    "        \n",
    "def upload():\n",
    "    \n",
    "    global filename\n",
    "    master.filename =  filedialog.askopenfilename(initialdir = \"/home/user/main_project/\",title = \"Select file\",filetypes = ((\"audio\",\"*.wav\"),(\"all files\",\"*.*\")))\n",
    "    print(master.filename)\n",
    "    global x\n",
    "    x=master.filename\n",
    "    print(\"x:\")\n",
    "    print(x)\n",
    "    fdetail = tk.Label(LFrame, text=\"FILE NAME\",font=(\"Arial\",18))\n",
    "    fname = tk.Label(LFrame, textvariable=x,font=(\"Arial\",18))\n",
    "    fdetail.place(x=200,y=210)\n",
    "    fname.place(x=500,y=210)\n",
    "\n",
    "    \n",
    "def detectemotion():\n",
    "    import pandas as pd\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    from keras.utils import np_utils\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "   \n",
    "    from keras.models import load_model\n",
    "    lmodel=load_model(\"Emotion_Voice_Detection_Model.h5\")\n",
    "    \n",
    "    lb = LabelEncoder()\n",
    "    lb.classes_ = np.load('classes.npy')\n",
    "    \n",
    "    X, sample_rate = librosa.load('output.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "    \n",
    "    featurelive = mfccs\n",
    "    livedf2 = featurelive\n",
    "    livedf2= pd.DataFrame(data=livedf2)\n",
    "    livedf2 = livedf2.stack().to_frame().T \n",
    "    \n",
    "    twodim= np.expand_dims(livedf2, axis=2)\n",
    "    \n",
    "    livepreds = lmodel.predict(twodim,batch_size=32,verbose=1)\n",
    "    livepreds1=livepreds.argmax(axis=1)\n",
    "    liveabc = livepreds1.astype(int).flatten()\n",
    "    print(livepreds)\n",
    "    livepredictions = (lb.inverse_transform((liveabc)))\n",
    "    print(livepredictions[0])\n",
    "    var.set(livepredictions[0])\n",
    " \n",
    "    \n",
    "label1 = Label(LFrame, text=\"UPLOAD VOICE\",font=(\"Arial\",18))\n",
    "b1 = Button(LFrame, text=\"BROWSE\", command=upload)\n",
    "label2 = Label(LFrame, text=\"DETECT EMOTION\",font=(\"Arial\",18))\n",
    "b2 = Button(LFrame, text=\"DETECT\", command=findemo)\n",
    "labeld = Label(LFrame,text=\"DETECTED EMOTION\",font=(\"Arial\",18))\n",
    "label3 = Label(LFrame,textvariable=emo,font=(\"Arial\",18))\n",
    "\n",
    "\n",
    "\n",
    "label4 = Label(LFrame, text=\"RECORD YOUR VOICE          \",font=(\"Arial\",18))\n",
    "b3 = Button(LFrame, text=\"RECORD\", command=callback)\n",
    "label5 = Label(LFrame, text=\"DETECT EMOTION        \",font=(\"Arial\",18))\n",
    "b4 = Button(LFrame, text=\"DETECT\",command=detectemotion)\n",
    "labeld1 = Label(LFrame,text=\"DETECTED EMOTION\",font=(\"Arial\",18))\n",
    "label6 = Label(LFrame,textvariable=var,font=(\"Arial\",18))\n",
    "b5 = Button(LFrame, text=\"EXIT\",command=master.destroy)\n",
    "\n",
    "title = tk.Label(topFrame, text=\"EMOTION DETECTION\",font=(\"Arial\",38),fg=\"white\",bg=\"black\")\n",
    "title.place(x=400,y=10)\n",
    "\n",
    "\n",
    "label1.place(x=200,y=160)\n",
    "b1.place(x=500,y=160)\n",
    "\n",
    "label2.place(x=200,y=250)\n",
    "b2.place(x=500,y=250)\n",
    "labeld.place(x=200,y=290)\n",
    "label3.place(x=500,y=290)\n",
    "\n",
    "\n",
    "b3.place(x=500,y=440)\n",
    "label4.place(x=200,y=440)\n",
    "b4.place(x=500,y=480)\n",
    "label5.place(x=200,y=480)\n",
    "b5.place(x=500,y=600)\n",
    "labeld1.place(x=200,y=520)\n",
    "label6.place(x=500,y=520)\n",
    "\n",
    "\n",
    "mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
